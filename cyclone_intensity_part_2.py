# -*- coding: utf-8 -*-
"""Cyclone Intensity Part 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BDb6-6jVlL0jGnCy8zkGRE02QSUcV5Ht
"""

from google.colab import drive
drive.mount('/content/drive')

from PIL import Image
import numpy as np

def image_to_pixels(image_path):
    img = Image.open('/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img')
    return np.array(img)

pip install opencv-python

import os
def read_images_to_pixels(image_folder):
    pixel_arrays = []
    for filename in os.listdir(image_folder):
        try:
            filepath = os.path.join(image_folder, filename)
            image = Image.open(filepath)
            pixel_array = np.array(image)
            pixel_arrays.append(pixel_array)
        except Exception as e:
            print(f"Error reading {filepath}: {e}")
    return np.array(pixel_arrays)
image_folder = "/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img"
pixels = read_images_to_pixels(image_folder)
print(pixels.shape)  # Example output: (num_images, height, width, channels)

import os
import cv2
import matplotlib.pyplot as plt

def calculate_histogram(img_path):
    img = cv2.imread(img_path)
    #For grayscale images, there's only one channel (0-indexed)
    hist = cv2.calcHist([img], [0], None, [256], [0, 255])
    return hist

def thresholding(folder_path):
    for filename in os.listdir(folder_path):
        if filename.endswith(".jpg") or filename.endswith(".png"):  # Adjust the file extensions as needed
            img_path = os.path.join(folder_path, filename)
            hist = calculate_histogram(img_path)

            plt.plot(hist)
            plt.title(f'Histogram for {filename}')
            plt.xlabel('Pixel Value')
            plt.ylabel('Frequency')
            plt.show()

if __name__ == '__main__':
    folder_path = "/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img"
    thresholding(folder_path)

import os
import cv2
import matplotlib.pyplot as plt

def calculate_histogram(img_path):
    img = cv2.imread(img_path)
    hist = cv2.calcHist([img], [0], None, [256], [0, 255])
    return hist

def thresholding(folder_path):
    overall_hist = None

    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            if filename.endswith((".jpg", ".png")):  # Adjust the file extensions as needed
                img_path = os.path.join(root, filename)
                hist = calculate_histogram(img_path)

                if overall_hist is None:
                    overall_hist = hist
                else:
                    overall_hist += hist

    plt.plot(overall_hist)
    plt.title('Overall Histogram for the Image Folder')
    plt.xlabel('Pixel Value')
    plt.ylabel('Frequency')
    plt.show()

if __name__ == '__main__':
    folder_path = "/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img"
    thresholding(folder_path)

import numpy as np

def gaussian_thresholding(image, sigma):


  # Apply a Gaussian filter to the image
  filtered_image = np.exp(-np.square(image - np.mean(image)) / (2 * sigma**2))

  # Calculate the mean of the filtered image
  mean = np.mean(filtered_image)

  # Apply thresholding
  thresholded_image = np.where(filtered_image > mean, 255, 0)

  return thresholded_image

# Example usage
image = np.random.randint(0, 256, size=(256, 256))  # Generate a random grayscale image
sigma = 10  # Set the standard deviation

thresholded_image = gaussian_thresholding(image, sigma)

# You can now process the thresholded_image further
print(thresholded_image.shape)



"""Otsu's method is another automatic thresholding technique used for image segmentation. It aims to find an optimal threshold value by maximizing the variance between two classes of pixels (foreground and background) in a grayscale image.

Applying multiple thresholds using adaptive thresholding involves segmenting an image into multiple regions, each with its own optimized threshold value. Here's a general approach you can follow:

1. Choose an Adaptive Thresholding Method:

Common methods include mean-C (average pixel intensity in a neighborhood minus a constant) and Gaussian-C (weighted average of neighborhood pixel intensities minus a constant). Choose based on your image characteristics and desired outcome.
2. Select Block Size and Constant:

Block size: This defines the neighborhood area used to calculate the threshold for each pixel. A larger size helps smooth out noise but might miss details. Start with a small size and increase it iteratively if needed.
Constant: This value is subtracted from the mean or weighted sum in the chosen adaptive method. A higher constant leads to a higher threshold, potentially losing details. Experiment with different values to find a balance.
"""

import cv2
import numpy as np
!pip install opencv-contrib-python
import cv2.ximgproc
from google.colab.patches import cv2_imshow
def apply_adaptive_threshold(image, method, block_size, c_value):
    if method == 'mean':
        return cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, block_size, c_value)
    elif method == 'gaussian':
        return cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, c_value)
    elif method == 'otsu':
        _, thresholded = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        return thresholded


image = cv2.imread('/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img/image4.jpg', cv2.IMREAD_GRAYSCALE)

# Parameters
block_size = 11
c_value = 2

# Apply adaptive thresholding for different methods
mean_thresholded = apply_adaptive_threshold(image, 'mean', block_size, c_value)
gaussian_thresholded = apply_adaptive_threshold(image, 'gaussian', block_size, c_value)
otsu_thresholded = apply_adaptive_threshold(image, 'otsu', block_size, c_value)


print("Threshold value for Adaptive Mean Thresholding:", np.mean(mean_thresholded))
print("Threshold value for Adaptive Gaussian Thresholding:",np.mean( gaussian_thresholded))
print("Threshold value for Adaptive otsu Thresholding:",np.mean( otsu_thresholded))

cv2.waitKey(0)
cv2.destroyAllWindows()

import cv2
import os
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans

def apply_adaptive_threshold(image, method, block_size, c_value):
    if method == 'mean':
        return cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, block_size, c_value)
    elif method == 'gaussian':
        return cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, c_value)
    elif method == 'otsu':
        _, thresholded = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        return thresholded

# Function to resize image
def resize_image(image, new_size):
    return cv2.resize(image, new_size)

# Folder path containing images
folder_path = '/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img'

# Parameters
block_size = 11
c_value = 2
new_image_size = (256, 256)  # New size for resizing images

# Lists to store data
image_names = []
mean_threshold_values = []
gaussian_threshold_values = []
otsu_threshold_values = []

# Loop through all images in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.jpg') or filename.endswith('.png'):  # Add more file extensions if needed
        image_path = os.path.join(folder_path, filename)

        # Read the image
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        # Resize the image
        resized_image = resize_image(image, new_image_size)

        # Apply adaptive thresholding for different methods
        mean_thresholded = apply_adaptive_threshold(resized_image, 'mean', block_size, c_value)
        gaussian_thresholded = apply_adaptive_threshold(resized_image, 'gaussian', block_size, c_value)
        otsu_thresholded = apply_adaptive_threshold(resized_image, 'otsu', block_size, c_value)

        # Store data in lists
        image_names.append(filename)
        mean_threshold_values.append(np.mean(mean_thresholded))
        gaussian_threshold_values.append(np.mean(gaussian_thresholded))
        otsu_threshold_values.append(np.mean(otsu_thresholded))

# Create a DataFrame
data = {
    'Image Name': image_names,
    'Mean Threshold': mean_threshold_values,
    'Gaussian Threshold': gaussian_threshold_values,
    'Otsu Threshold': otsu_threshold_values
}
df = pd.DataFrame(data)

# Perform clustering using KMeans
num_clusters = 3  # Change the number of clusters as needed
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
df['Cluster'] = kmeans.fit_predict(df[['Mean Threshold', 'Gaussian Threshold', 'Otsu Threshold']])

# Save the DataFrame to a CSV file
csv_file_path = '/content/drive/MyDrive/major_project/threshold_data.csv'
df.to_csv(csv_file_path, index=False)

print(f"Data saved to {csv_file_path}")

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split


data = pd.read_csv("/content/drive/MyDrive/major_project/threshold_data.csv")

data.head(10)

# @title Comparison of Threshold Types

import matplotlib.pyplot as plt
_ = plt.scatter(data['Mean Threshold'], data['Gaussian Threshold'], c=data['Otsu Threshold'])

# new_image = cv2.imread('/content/drive/MyDrive/BE_MAJOR/normalized_img/img91.jpg')
# new_image = cv2.resize(new_image, (128, 128))  # Resize image if needed
# new_image = np.expand_dims(new_image, axis=0)  # Add batch dimension
# predicted_intensity = model.predict(new_image)
# print(f'Predicted Intensity: {predicted_intensity[0]}')

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
train=pd.read_csv("/content/drive/MyDrive/major_project/threshold_data.csv")

train_dir="/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img"
train_datagen=ImageDataGenerator(rescale=1./255)
train_data=train_datagen.flow_from_dataframe(train,train_dir,
                                             x_col='Image Name',
                                             y_col="Cluster",
                                             subset='training',
                                             target_size=(256,256),
                                             batch_size=16,
                                             class_mode="raw"
                                             )

from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout,Input
from tensorflow.keras import regularizers
from tensorflow.keras.models import Model
inputs=Input(shape=(256,256,3))
y=Conv2D(256,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(inputs)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=Conv2D(256,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=MaxPool2D()(y)

y=Conv2D(256,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=Conv2D(128,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=MaxPool2D()(y)

y=Conv2D(128,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=Conv2D(64,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=MaxPool2D()(y)

y=Conv2D(64,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=Conv2D(32,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=MaxPool2D()(y)

y=Conv2D(32,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=Conv2D(16,3,activation=None, padding="same",kernel_initializer="he_normal",kernel_regularizer=regularizers.L1L2(0.01))(y)
y=tf.keras.layers.BatchNormalization()(y)
y=tf.keras.activations.relu(y)
y=MaxPool2D()(y)

y= Flatten()(y)
outputs=Dense(1,activation='linear')(y)
model_1=Model(inputs=inputs,outputs=outputs)

model_1.summary()

model_1.compile(loss=tf.keras.losses.mse,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),
                metrics=["mse"])
early_stopping=tf.keras.callbacks.EarlyStopping(monitor="loss",patience=10,mode='min')

history_1=model_1.fit(train_data,
                      epochs=5,callbacks=[early_stopping])

plt.plot(history_1.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

def load_and_prep_image(filename, img_shape=256):

  img = tf.io.read_file(filename)
  img = tf.image.decode_image(img, channels=3)
  img = tf.image.resize(img, size = [img_shape, img_shape])
  img = img/255.
  return img

def pred_and_plot(model, filename):
  img = load_and_prep_image(filename)

  # Make a prediction
  pred = model.predict(tf.expand_dims(img, axis=0))

# Plot the image and predicted value
  plt.imshow(img)
  plt.title(f"Prediction: {pred}")
  plt.axis(False);

pred_and_plot(model_1,"/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img/img129.jpg")

pred_and_plot(model_1,"/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img/img130.jpg")

pred_and_plot(model_1,"/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img/img160.jpg")

pred_and_plot(model_1,"/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img/20240010051_GOES16-GLM-CONUS-EXTENT3-2500x1500.jpg")

pred_and_plot(model_1,"/content/drive/MyDrive/major_project/be_major/Major 2/normalized_img/img85.jpg")

import pickle
filename= "Trained_model.sav"
a=open(filename,'wb')
pickle.dump(model_1,a)
a.flush()
a.close()

from joblib import dump

# Save the trained model
dump(model_1, 'model.joblib')

with open('model.pkl', 'wb') as f:
    pickle.dump(model_1, f)

from joblib import load

# Load the trained model
model = load('model.joblib')

import tensorflow as tf

# Assuming model_1 is your Keras model
model_1.save("Trained_model1.h5")

# Now you can load the model using the load_model function
loaded_model = tf.keras.models.load_model('Trained_model1.h5')

from google.colab import files
files.download(filename)

import pandas as pd
df=pd.read_csv("/content/drive/MyDrive/major_project/feature_wind.csv")

from google.colab import drive
drive.mount('/content/drive')

df.head(5)

df.shape

df.size

df.info()

df.describe()

df.isnull().sum()

df.isnull().sum().sum()

df.tail(10)

df1=df.drop(['#','Name','Date','Time'],axis=1)

df1.head(4)

df1['Landfall_Country'].unique()

df1.info()

df1['Longitude'].fillna(df1['Longitude'].mean(),inplace=True)

df1['Latitude '].fillna(df1['Latitude '].mean(),inplace=True)

df1['max_winds(kt)'].fillna(df1['max_winds(kt)'].mean() ,inplace=True)

df1['Landfall_Country'].bfill(inplace=True)

df1.isnull().sum().sum()

df1.isnull().sum()

df1.groupby("Landfall_Country")["Landfall_Country"].agg("count")

import numpy as np
df1['max_winds(kt)'].max()

df1['Longitude'].max()

df1['Latitude '].max()

df1.tail(5)

len(df1['Landfall_Country'].unique())

df1['Landfall_Country']=df1['Landfall_Country'].apply(lambda x: x.strip())
location_stat=df1.groupby("Landfall_Country")['Landfall_Country'].agg("count")
location_stat

dummies=pd.get_dummies(df1['Landfall_Country'])
dummies

y=df1['max_winds(kt)']

X=df1.drop(['max_winds(kt)'],axis=1)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df1['Landfall_Country']= le.fit_transform(df1['Landfall_Country'])

df1['Landfall_Country'].unique()

data=pd.concat([df1,dummies],axis=1)

data.head(5)

data.drop(['Landfall_Country'],axis=1,inplace=True)

data.head(5)

x=data.drop(['max_winds(kt)'],axis=1)

y=data['max_winds(kt)']

x.shape

y.shape

y.head(5)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.05)

x_train.shape

x_test.shape

y_train.shape

from sklearn.ensemble import RandomForestRegressor
model=RandomForestRegressor(n_estimators=5000)
model.fit(x_train,y_train)

model.score(x_test,y_test)

def predict(Landfall_Country, Latitude, Longitude):
    loc_index = np.where(x.columns == Landfall_Country)[0][0]

    x1 = np.zeros(len(x.columns))
    x1[0] = Latitude
    x1[1] = Longitude
    if loc_index >=0:
        loc_index=1
    return model.predict([x1])[0]

predict("Cuba",21.6,82.6)

predict("Cuba",22.2,83.6)

predict("Honduras",15.6,84.1)

def estimate_distance(latitude, longitude, wind_speed, intensity, time_interval):
    # Assuming a constant speed over the interval, influenced by intensity
    # You might want to refine this with more sophisticated models
    # Convert wind speed from knots to meters per second
    speed_mps = wind_speed * 0.514444

    # Adjust speed based on intensity (example: simple linear relationship)
    adjusted_speed_mps = speed_mps * (1 + intensity * 0.1)  # Adjusting speed based on intensity

    distance_m = adjusted_speed_mps * time_interval  # Distance = Speed * Time
    return distance_m / 1000  # Convert meters to kilometers

# Example usage
latitude = 22.1  # Example latitude
longitude = 97.8  # Example longitude
wind_speed_knots = 35  # Example wind speed in knots
intensity = 0.167  # Example intensity (can be a value between 0 and 1)
time_interval_hours = 1.0  # Example time interval in hours

distance = estimate_distance(latitude, longitude, wind_speed_knots, intensity, time_interval_hours * 3600)  # Convert hours to seconds
print("Estimated distance traveled by cyclone:", distance, "kilometers")

